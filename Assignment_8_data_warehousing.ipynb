{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Data Warehousing Fundamentals\n",
        "   1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
        "   2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
        "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "tzn5R--80b7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.To design a data warehouse schema for a retail company, we will create dimension tables for products, customers, and time. We will also create a fact table to capture sales data. For this example, we will use a relational database management system (RDBMS) and assume the following table structures:\n",
        "\n",
        "Product Dimension Table:\n",
        "\n",
        "ProductID (Primary Key)\n",
        "ProductName\n",
        "Category\n",
        "Price\n",
        "...\n",
        "Customer Dimension Table:\n",
        "\n",
        "CustomerID (Primary Key)\n",
        "FirstName\n",
        "LastName\n",
        "Address\n",
        "...\n",
        "Time Dimension Table:\n",
        "\n",
        "DateKey (Primary Key)\n",
        "Date\n",
        "Year\n",
        "Month\n",
        "Day\n",
        "...\n",
        "Sales Fact Table:\n",
        "\n",
        "SaleID (Primary Key)\n",
        "ProductID (Foreign Key to Product Dimension Table)\n",
        "CustomerID (Foreign Key to Customer Dimension Table)\n",
        "DateKey (Foreign Key to Time Dimension Table)\n",
        "SalesAmount"
      ],
      "metadata": {
        "id": "RgyAtoUq25W9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Product Dimension Table:\n",
        "\n",
        "ProductID\tProductName\tCategory\tPrice\n",
        "1\tLaptop\tElectronics\t1000\n",
        "2\tSmartphone\tElectronics\t800\n",
        "3\tT-Shirt\tClothing\t20\n",
        "4\tShoes\tClothing\t50\n",
        "...\t...\t...\t...\n",
        "Customer Dimension Table:\n",
        "\n",
        "CustomerID\tFirstName\tLastName\tAddress\n",
        "1\tJohn\tDoe\t123 Main St.\n",
        "2\tJane\tSmith\t456 Elm St.\n",
        "3\tDavid\tJohnson\t789 Oak St.\n",
        "4\tEmily\tDavis\t987 Pine St.\n",
        "...\t...\t...\t...\n",
        "Time Dimension Table:\n",
        "\n",
        "DateKey\tDate\tYear\tMonth\tDay\n",
        "1\t2023-01-01\t2023\t01\t01\n",
        "2\t2023-01-02\t2023\t01\t02\n",
        "3\t2023-01-03\t2023\t01\t03\n",
        "4\t2023-01-04\t2023\t01\t04\n",
        "...\t...\t...\t...\t...\n",
        "Now, let's populate the fact table with sample sales data:\n",
        "\n",
        "Sales Fact Table:\n",
        "\n",
        "SaleID\tProductID\tCustomerID\tDateKey\tSalesAmount\n",
        "1\t1\t1\t1\t1500\n",
        "2\t2\t2\t1\t800\n",
        "3\t3\t3\t2\t60\n",
        "4\t4\t4\t2\t100\n",
        "...\t...\t...\t...\t..."
      ],
      "metadata": {
        "id": "oJUyRmTK3JqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZRVg36B0bIf"
      },
      "outputs": [],
      "source": [
        "SELECT\n",
        "    p.ProductName,\n",
        "    SUM(f.SalesAmount) AS TotalSalesAmount\n",
        "FROM\n",
        "    SalesFactTable f\n",
        "    INNER JOIN ProductDimensionTable p ON f.ProductID = p.ProductID\n",
        "GROUP BY\n",
        "    p.ProductName;\n",
        "\n",
        "SELECT\n",
        "    c.FirstName,\n",
        "    c.LastName,\n",
        "    SUM(f.SalesAmount) AS TotalSalesAmount\n",
        "FROM\n",
        "    SalesFactTable f\n",
        "    INNER JOIN CustomerDimensionTable c ON f.CustomerID = c.CustomerID\n",
        "    INNER JOIN TimeDimensionTable t ON f.DateKey = t.DateKey\n",
        "WHERE\n",
        "    t.Year = 2023\n",
        "GROUP BY\n",
        "    c.FirstName,\n",
        "    c.LastName;\n",
        "\n",
        "SELECT\n",
        "    t.Year,\n",
        "    t.Month,\n",
        "    SUM(f.SalesAmount) AS TotalSalesAmount\n",
        "FROM\n",
        "    SalesFactTable f\n",
        "    INNER JOIN TimeDimensionTable t ON f.DateKey = t.DateKey\n",
        "GROUP BY\n",
        "    t.Year,\n",
        "    t.Month;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: ETL and Data Integration\n",
        "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
        "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nnGOS-dR052-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import sqlite3\n",
        "\n",
        "def extract_data(file_path):\n",
        "    with open(file_path, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        header = next(csv_reader)\n",
        "        data = [row for row in csv_reader]\n",
        "    return data\n",
        "\n",
        "def transform_data(data):\n",
        "    transformed_data = []\n",
        "    for row in data:\n",
        "        product_id = int(row[0])\n",
        "        sales_amount = float(row[1])\n",
        "        transformed_data.append((product_id, sales_amount))\n",
        "    return transformed_data\n",
        "\n",
        "def load_data(data):\n",
        "    conn = sqlite3.connect('data_warehouse.db')\n",
        "    cursor = conn.cursor()\n",
        "    for row in data:\n",
        "        product_id, sales_amount = row\n",
        "        cursor.execute(\"INSERT INTO SalesFactTable (ProductID, SalesAmount) VALUES (?, ?)\",\n",
        "                       (product_id, sales_amount))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "file_path = 'sales_data.csv'\n",
        "extracted_data = extract_data(file_path)\n",
        "transformed_data = transform_data(extracted_data)\n",
        "load_data(transformed_data)\n"
      ],
      "metadata": {
        "id": "tCfjQoch08E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Dimensional Modeling and Schemas\n",
        "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
        "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n"
      ],
      "metadata": {
        "id": "zCnJIhor2W0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To design a star schema for a university database, we will create a fact table for student enrollments and dimension tables for students, courses, and time. For this example, let's assume the following table structures:\n",
        "\n",
        "Student Dimension Table:\n",
        "\n",
        "StudentID (Primary Key)\n",
        "FirstName\n",
        "LastName\n",
        "Age\n",
        "Gender\n",
        "...\n",
        "Course Dimension Table:\n",
        "\n",
        "CourseID (Primary Key)\n",
        "CourseName\n",
        "Department\n",
        "Professor\n",
        "...\n",
        "Time Dimension Table:\n",
        "\n",
        "DateKey (Primary Key)\n",
        "Date\n",
        "Year\n",
        "Month\n",
        "Day\n",
        "...\n",
        "Enrollment Fact Table:\n",
        "\n",
        "EnrollmentID (Primary Key)\n",
        "StudentID (Foreign Key to Student Dimension Table)\n",
        "CourseID (Foreign Key to Course Dimension Table)\n",
        "DateKey (Foreign Key to Time Dimension Table)\n",
        "Grade"
      ],
      "metadata": {
        "id": "DjxxHoWH3uR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT\n",
        "    c.CourseName,\n",
        "    COUNT(e.EnrollmentID) AS TotalEnrollments\n",
        "FROM\n",
        "    EnrollmentFactTable e\n",
        "    INNER JOIN CourseDimensionTable c ON e.CourseID = c.CourseID\n",
        "GROUP BY\n",
        "    c.CourseName;\n",
        "\n",
        "SELECT\n",
        "    c.CourseName,\n",
        "    SUM(CASE WHEN s.Gender = 'M' THEN 1 ELSE 0 END) AS MaleStudents,\n",
        "    SUM(CASE WHEN s.Gender = 'F' THEN 1 ELSE 0 END) AS FemaleStudents\n",
        "FROM\n",
        "    EnrollmentFactTable e\n",
        "    INNER JOIN CourseDimensionTable c ON e.CourseID = c.CourseID\n",
        "    INNER JOIN StudentDimensionTable s ON e.StudentID = s.StudentID\n",
        "GROUP BY\n",
        "    c.CourseName;\n",
        "\n",
        "SELECT\n",
        "    c.CourseName,\n",
        "    AVG(e.Grade) AS AverageGrade\n",
        "FROM\n",
        "    EnrollmentFactTable e\n",
        "    INNER JOIN CourseDimensionTable c ON e.CourseID = c.CourseID\n",
        "    INNER JOIN TimeDimensionTable t ON e.DateKey = t.DateKey\n",
        "WHERE\n",
        "    t.Year = 2023\n",
        "GROUP BY\n",
        "    c.CourseName;\n"
      ],
      "metadata": {
        "id": "TZFePuXU2bUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Performance Optimization and Querying\n",
        "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
        "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
        "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
        "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n"
      ],
      "metadata": {
        "id": "5XGnfXrD2b0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import psycopg2\n",
        "from multiprocessing import Pool\n",
        "from datetime import datetime\n",
        "\n",
        "file_path = 'data.csv'\n",
        "batch_size = 1000\n",
        "num_processes = 4\n",
        "\n",
        "def load_data(connection, cursor, data):\n",
        "    query = \"COPY SalesFactTable (ProductID, SalesAmount) FROM STDIN WITH CSV\"\n",
        "    cursor.copy_expert(query, data)\n",
        "    connection.commit()\n",
        "\n",
        "def process_batch(batch):\n",
        "    connection = psycopg2.connect(\n",
        "        host='your_host',\n",
        "        port='your_port',\n",
        "        database='your_database',\n",
        "        user='your_username',\n",
        "        password='your_password'\n",
        "    )\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    try:\n",
        "        load_data(connection, cursor, batch)\n",
        "    except (Exception, psycopg2.DatabaseError) as error:\n",
        "        print(f\"Error: {error}\")\n",
        "    finally:\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "\n",
        "def load_data_optimized():\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    with open(file_path, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        header = next(csv_reader)\n",
        "\n",
        "        data_batches = [list(csv_reader[i:i + batch_size]) for i in range(0, len(csv_reader), batch_size)]\n",
        "\n",
        "        pool = Pool(processes=num_processes)\n",
        "        pool.map(process_batch, data_batches)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    loading_time = end_time - start_time\n",
        "    print(f\"Data loading completed in: {loading_time}\")\n",
        "\n",
        "print(\"Loading time without optimizations:\")\n",
        "load_data_optimized()\n",
        "\n",
        "print(\"Loading time with optimizations:\")\n",
        "load_data_optimized()\n"
      ],
      "metadata": {
        "id": "iepRjPEY2f-3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}