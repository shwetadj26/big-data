{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
        "\n"
      ],
      "metadata": {
        "id": "nGlfCGefhNMC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fTtDi6hdhLv7"
      },
      "outputs": [],
      "source": [
        "def read_hadoop_config(file_path):\n",
        "    components = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith('<name>'):\n",
        "                component = line.lstrip('<name>').rstrip('</name>')\n",
        "                components.append(component)\n",
        "    return components\n",
        "\n",
        "config_file = '/path/to/hadoop/conf/hadoop-site.xml'\n",
        "core_components = read_hadoop_config(config_file)\n",
        "print(\"Core components of Hadoop:\")\n",
        "for component in core_components:\n",
        "    print(component)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
      ],
      "metadata": {
        "id": "2bM3d7PwhSCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def get_directory_size(hdfs_url, directory_path):\n",
        "    client = InsecureClient(hdfs_url)\n",
        "    total_size = 0\n",
        "\n",
        "    for file_status in client.list(directory_path, status=True):\n",
        "        if file_status['type'] == 'DIRECTORY':\n",
        "            subdirectory_path = directory_path + '/' + file_status['pathSuffix']\n",
        "            total_size += get_directory_size(hdfs_url, subdirectory_path)\n",
        "        else:\n",
        "            total_size += file_status['length']\n",
        "\n",
        "    return total_size\n",
        "\n",
        "hdfs_url = 'http://localhost:9870'\n",
        "directory_path = '/user/username/data'\n",
        "total_size = get_directory_size(hdfs_url, directory_path)\n",
        "print(\"Total file size in HDFS directory:\", total_size, \"bytes\")\n"
      ],
      "metadata": {
        "id": "V9QJuhMthVI4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
      ],
      "metadata": {
        "id": "G7-sr-2MhVm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import multiprocessing\n",
        "\n",
        "def map_reduce_word_count(file_path, num_top_words):\n",
        "    with open(file_path, 'r') as file:\n",
        "        pool = multiprocessing.Pool()\n",
        "        results = pool.map(count_words, file)\n",
        "\n",
        "    word_counts = Counter()\n",
        "    for result in results:\n",
        "        word_counts += Counter(result)\n",
        "\n",
        "    top_words = word_counts.most_common(num_top_words)\n",
        "    for word, count in top_words:\n",
        "        print(word, count)\n",
        "\n",
        "def count_words(line):\n",
        "    words = line.strip().split()\n",
        "    return Counter(words)\n",
        "\n",
        "file_path = '/path/to/large_text_file.txt'\n",
        "num_top_words = 10\n",
        "map_reduce_word_count(file_path, num_top_words)\n"
      ],
      "metadata": {
        "id": "iEROsrjXhYGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
      ],
      "metadata": {
        "id": "G6DFZbQNhYil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def check_hadoop_health(namenode_host, namenode_port):\n",
        "    namenode_url = f\"http://{namenode_host}:{namenode_port}\"\n",
        "    namenode_health_url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
        "    datanode_health_url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=DataNodeInfo\"\n",
        "\n",
        "    namenode_health_response = requests.get(namenode_health_url).json()\n",
        "    datanode_health_response = requests.get(datanode_health_url).json()\n",
        "\n",
        "    namenode_status = namenode_health_response['beans'][0]['State']\n",
        "    datanode_status = datanode_health_response['beans'][0]['State']\n",
        "\n",
        "    print(\"NameNode status:\", namenode_status)\n",
        "    print(\"DataNode status:\", datanode_status)\n",
        "\n",
        "namenode_host = 'localhost'\n",
        "namenode_port = 50070\n",
        "check_hadoop_health(namenode_host, namenode_port)\n"
      ],
      "metadata": {
        "id": "ZXvKgHmkha8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n"
      ],
      "metadata": {
        "id": "_hOg9IdahbYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def list_hdfs_path(hdfs_url, path):\n",
        "    client = InsecureClient(hdfs_url)\n",
        "    items = client.list(path, status=True)\n",
        "\n",
        "    for item in items:\n",
        "        item_type = item['type']\n",
        "        item_path = item['path']\n",
        "        print(f\"{item_type}: {item_path}\")\n",
        "\n",
        "hdfs_url = 'http://localhost:9870'\n",
        "path = '/user/username/data'\n",
        "list_hdfs_path(hdfs_url, path)\n"
      ],
      "metadata": {
        "id": "12P0AB-ZhdrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
        "."
      ],
      "metadata": {
        "id": "RCakntihheHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def analyze_datanode_storage_utilization(namenode_host, namenode_port):\n",
        "    namenode_url = f\"http://{namenode_host}:{namenode_port}\"\n",
        "    datanode_info_url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
        "\n",
        "    datanode_info_response = requests.get(datanode_info_url).json()\n",
        "    datanodes = datanode_info_response['beans'][0]['LiveNodes']\n",
        "\n",
        "    storage_capacities = []\n",
        "    for datanode_id, datanode_info in datanodes.items():\n",
        "        storage_capacity = datanode_info['capacity']\n",
        "        storage_capacities.append((datanode_id, storage_capacity))\n",
        "\n",
        "    sorted_capacities = sorted(storage_capacities, key=lambda x: x[1])\n",
        "\n",
        "    print(\"DataNodes with the highest storage capacities:\")\n",
        "    for datanode_id, storage_capacity in sorted_capacities[-3:]:\n",
        "        print(f\"DataNode: {datanode_id}, Storage Capacity: {storage_capacity} bytes\")\n",
        "\n",
        "    print(\"\\nDataNodes with the lowest storage capacities:\")\n",
        "    for datanode_id, storage_capacity in sorted_capacities[:3]:\n",
        "        print(f\"DataNode: {datanode_id}, Storage Capacity: {storage_capacity} bytes\")\n",
        "\n",
        "namenode_host = 'localhost'\n",
        "namenode_port = 50070\n",
        "analyze_datanode_storage_utilization(namenode_host, namenode_port)\n"
      ],
      "metadata": {
        "id": "UtAbV6n6hghc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
      ],
      "metadata": {
        "id": "mHviLFd4hg5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job(yarn_rm_host, yarn_rm_port, job_params):\n",
        "    submit_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/cluster/apps/new-application\"\n",
        "    response = requests.post(submit_url)\n",
        "    app_id = response.json()['application-id']\n",
        "\n",
        "    submit_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/cluster/apps/{app_id}/submit\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    response = requests.post(submit_url, json=job_params, headers=headers)\n",
        "\n",
        "    if response.status_code == 202:\n",
        "        print(\"Hadoop job submitted successfully.\")\n",
        "        print(\"Application ID:\", app_id)\n",
        "\n",
        "        tracking_url = response.json()['app']['trackingUrl']\n",
        "        print(\"Tracking URL:\", tracking_url)\n",
        "\n",
        "        while True:\n",
        "            status_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/cluster/apps/{app_id}\"\n",
        "            status_response = requests.get(status_url)\n",
        "            status = status_response.json()['app']['state']\n",
        "            print(\"Job status:\", status)\n",
        "\n",
        "            if status == 'FINISHED' or status == 'FAILED' or status == 'KILLED':\n",
        "                break\n",
        "\n",
        "            time.sleep(5)\n",
        "\n",
        "        if status == 'FINISHED':\n",
        "            output_url = f\"{tracking_url}/ws/v1/mapreduce/jobs/{app_id}/jobattempts\"\n",
        "            output_response = requests.get(output_url)\n",
        "            output = output_response.json()['jobAttempts']['jobAttempt'][0]['logsLink']\n",
        "            print(\"Job output:\", output)\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to submit the Hadoop job.\")\n",
        "\n",
        "yarn_rm_host = 'localhost'\n",
        "yarn_rm_port = 8088\n",
        "job_params = {\n",
        "    \"application-id\": \"application_1621203960916_0001\",\n",
        "    \"application-name\": \"WordCount\",\n",
        "    \"am-container-spec\": {\n",
        "        \"commands\": {\n",
        "            \"command\": \"hadoop jar WordCount.jar input output\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "submit_hadoop_job(yarn_rm_host, yarn_rm_port, job_params)\n"
      ],
      "metadata": {
        "id": "u0L_LCCThj3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
      ],
      "metadata": {
        "id": "JIaK_dUShkOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job_with_resource_tracking(yarn_rm_host, yarn_rm_port, job_params):\n",
        "    submit_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/cluster/apps/new-application\"\n",
        "    response = requests.post(submit_url)\n",
        "    app_id = response.json()['application-id']\n",
        "\n",
        "    submit_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/cluster/apps/{app_id}/submit\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    response = requests.post(submit_url, json=job_params, headers=headers)\n",
        "\n",
        "    if response.status_code == 202:\n",
        "        print(\"Hadoop job submitted successfully.\")\n",
        "        print(\"Application ID:\", app_id)\n",
        "\n",
        "        while True:\n",
        "            status_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/cluster/apps/{app_id}\"\n",
        "            status_response = requests.get(status_url)\n",
        "            status = status_response.json()['app']['state']\n",
        "            print(\"Job status:\", status)\n",
        "\n",
        "            if status == 'FINISHED' or status == 'FAILED' or status == 'KILLED':\n",
        "                break\n",
        "\n",
        "            resources_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/cluster/apps/{app_id}/metrics/resource-usage\"\n",
        "            resources_response = requests.get(resources_url)\n",
        "            resources = resources_response.json()['apps']['app'][0]['resources']\n",
        "            print(\"Resource usage:\", resources)\n",
        "\n",
        "            time.sleep(5)\n",
        "\n",
        "        if status == 'FINISHED':\n",
        "            output_url = f\"http://{yarn_rm_host}:{yarn_rm_port}/ws/v1/mapreduce/jobs/{app_id}/jobattempts\"\n",
        "            output_response = requests.get(output_url)\n",
        "            output = output_response.json()['jobAttempts']['jobAttempt'][0]['logsLink']\n",
        "            print(\"Job output:\", output)\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to submit the Hadoop job.\")\n",
        "\n",
        "yarn_rm_host = 'localhost'\n",
        "yarn_rm_port = 8088\n",
        "job_params = {\n",
        "    \"application-id\": \"application_1621203960916_0001\",\n",
        "    \"application-name\": \"WordCount\",\n",
        "    \"am-container-spec\": {\n",
        "        \"commands\": {\n",
        "            \"command\": \"hadoop jar WordCount.jar input output\"\n",
        "        }\n",
        "    },\n",
        "    \"resource\": {\n",
        "        \"vcores\": 2,\n",
        "        \"memory\": 2048\n",
        "    }\n",
        "}\n",
        "submit_hadoop_job_with_resource_tracking(yarn_rm_host, yarn_rm_port, job_params)\n"
      ],
      "metadata": {
        "id": "PUGY4IUbhmgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time"
      ],
      "metadata": {
        "id": "yQNznXrMhm3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_mapreduce_job(input_path, output_path, split_size):\n",
        "    start_time = time.time()\n",
        "\n",
        "    hadoop_command = [\n",
        "        'hadoop', 'jar', 'example.jar', 'ExampleJob',\n",
        "        '-D', f'mapreduce.input.fileinputformat.split.minsize={split_size}',\n",
        "        input_path, output_path\n",
        "    ]\n",
        "    subprocess.run(hadoop_command, check=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Input split size: {split_size}, Execution time: {execution_time} seconds\")\n",
        "\n",
        "input_path = '/input/data.txt'\n",
        "output_path = '/output/result'\n",
        "split_sizes = [128 * 1024 * 1024, 256 * 1024 * 1024, 512 * 1024 * 1024]\n",
        "\n",
        "for split_size in split_sizes:\n",
        "    run_mapreduce_job(input_path, output_path, split_size)\n"
      ],
      "metadata": {
        "id": "PJSZlnTmhpEd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}