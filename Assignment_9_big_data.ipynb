{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Docker\n",
        "1. Scenario: You are building a microservices-based application using Docker. Design a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. Ensure that the containers can communicate with each other properly.\n",
        "2. Scenario: You want to scale your Docker containers dynamically based on the incoming traffic. Write a Python script that utilizes Docker SDK to monitor the CPU usage of a container and automatically scales the number of replicas based on a threshold.\n",
        "3. Scenario: You have a Docker image stored on a private registry. Develop a script in Bash that authenticates with the registry, pulls the latest version of the image, and runs a container based on that image.\n"
      ],
      "metadata": {
        "id": "izJ2SJbe5Tpf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlKIvIvU5S6l"
      },
      "outputs": [],
      "source": [
        "version: '3'\n",
        "services:\n",
        "  web:\n",
        "    build: ./web\n",
        "    ports:\n",
        "      - \"80:80\"\n",
        "    depends_on:\n",
        "      - database\n",
        "      - cache\n",
        "  database:\n",
        "    image: mysql:latest\n",
        "    environment:\n",
        "      - MYSQL_ROOT_PASSWORD=your_password\n",
        "  cache:\n",
        "    image: redis:latest\n",
        "\n",
        "import docker\n",
        "import psutil\n",
        "\n",
        "client = docker.from_env()\n",
        "\n",
        "def get_container_cpu_percent(container_id):\n",
        "    container = client.containers.get(container_id)\n",
        "    stats = container.stats(stream=False)\n",
        "    cpu_stats = stats['cpu_stats']\n",
        "    cpu_usage = cpu_stats['cpu_usage']['total_usage']\n",
        "    system_cpu_usage = cpu_stats['system_cpu_usage']\n",
        "    cpu_percent = (cpu_usage / system_cpu_usage) * 100\n",
        "    return cpu_percent\n",
        "\n",
        "def scale_containers(container_id, replicas, threshold):\n",
        "    cpu_percent = get_container_cpu_percent(container_id)\n",
        "    if cpu_percent > threshold:\n",
        "        print(f\"CPU usage ({cpu_percent}%) exceeded the threshold. Scaling up...\")\n",
        "        scale_up(container_id, replicas + 1)\n",
        "    else:\n",
        "        print(f\"CPU usage ({cpu_percent}%) is below the threshold. No scaling required.\")\n",
        "\n",
        "def scale_up(container_id, replicas):\n",
        "    container = client.containers.get(container_id)\n",
        "    container.scale(replicas)\n",
        "\n",
        "container_id = 'your_container_id'\n",
        "replicas = 3\n",
        "threshold = 80\n",
        "\n",
        "scale_containers(container_id, replicas, threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docker\n",
        "import psutil\n",
        "\n",
        "client = docker.from_env()\n",
        "\n",
        "def get_container_cpu_percent(container_id):\n",
        "    container = client.containers.get(container_id)\n",
        "    stats = container.stats(stream=False)\n",
        "    cpu_stats = stats['cpu_stats']\n",
        "    cpu_usage = cpu_stats['cpu_usage']['total_usage']\n",
        "    system_cpu_usage = cpu_stats['system_cpu_usage']\n",
        "    cpu_percent = (cpu_usage / system_cpu_usage) * 100\n",
        "    return cpu_percent\n",
        "\n",
        "def scale_containers(container_id, replicas, threshold):\n",
        "    cpu_percent = get_container_cpu_percent(container_id)\n",
        "    if cpu_percent > threshold:\n",
        "        print(f\"CPU usage ({cpu_percent}%) exceeded the threshold. Scaling up...\")\n",
        "        scale_up(container_id, replicas + 1)\n",
        "    else:\n",
        "        print(f\"CPU usage ({cpu_percent}%) is below the threshold. No scaling required.\")\n",
        "\n",
        "def scale_up(container_id, replicas):\n",
        "    container = client.containers.get(container_id)\n",
        "    container.scale(replicas)\n",
        "\n",
        "container_id = 'your_container_id'\n",
        "replicas = 3\n",
        "threshold = 80\n",
        "\n",
        "scale_containers(container_id, replicas, threshold)\n"
      ],
      "metadata": {
        "id": "hDqAXIFM6LXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DOCKER_REGISTRY_URL=\"your_registry_url\"\n",
        "DOCKER_REGISTRY_USERNAME=\"your_username\"\n",
        "DOCKER_REGISTRY_PASSWORD=\"your_password\"\n",
        "DOCKER_IMAGE_NAME=\"your_image_name\"\n",
        "\n",
        "docker login -u \"$DOCKER_REGISTRY_USERNAME\" -p \"$DOCKER_REGISTRY_PASSWORD\" \"$DOCKER_REGISTRY_URL\"\n",
        "docker pull \"$DOCKER_REGISTRY_URL/$DOCKER_IMAGE_NAME\"\n",
        "docker run -d \"$DOCKER_REGISTRY_URL/$DOCKER_IMAGE_NAME\"\n"
      ],
      "metadata": {
        "id": "MQ1rkvni6PgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Airflow\n",
        "1. Scenario: You have a data pipeline that requires executing a shell command as part of a task. Create an Airflow DAG that includes a BashOperator to execute a specific shell command.\n",
        "2. Scenario: You want to create dynamic tasks in Airflow based on a list of inputs. Design an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list.\n",
        "3. Scenario: You need to set up a complex task dependency in Airflow, where Task B should start only if Task A has successfully completed. Implement this dependency using the \"TriggerDagRunOperator\" in Airflow.\n"
      ],
      "metadata": {
        "id": "sVsYGAeR6WRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "dag = DAG(\n",
        "    'execute_shell_command',\n",
        "    description='DAG to execute a shell command',\n",
        "    schedule_interval=None,\n",
        "    start_date=datetime(2023, 7, 18),\n",
        "    catchup=False\n",
        ")\n",
        "\n",
        "execute_command = BashOperator(\n",
        "    task_id='execute_command',\n",
        "    bash_command='your_shell_command_here',\n",
        "    dag=dag\n",
        ")\n"
      ],
      "metadata": {
        "id": "qj8BhdV76oLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "dag = DAG(\n",
        "    'dynamic_tasks',\n",
        "    description='DAG to generate tasks dynamically',\n",
        "    schedule_interval=None,\n",
        "    start_date=datetime(2023, 7, 18),\n",
        "    catchup=False\n",
        ")\n",
        "\n",
        "def process_input(input_value):\n",
        "    print(f\"Processing input: {input_value}\")\n",
        "\n",
        "input_list = ['input1', 'input2', 'input3']\n",
        "\n",
        "for input_value in input_list:\n",
        "    task_id = f'task_{input_value}'\n",
        "    task = PythonOperator(\n",
        "        task_id=task_id,\n",
        "        python_callable=process_input,\n",
        "        op_kwargs={'input_value': input_value},\n",
        "        dag=dag\n",
        "    )\n"
      ],
      "metadata": {
        "id": "22nJiNt-6r6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.dagrun_operator import TriggerDagRunOperator\n",
        "from datetime import datetime\n",
        "\n",
        "dag = DAG(\n",
        "    'complex_task_dependency',\n",
        "    description='DAG with complex task dependency',\n",
        "    schedule_interval=None,\n",
        "    start_date=datetime(2023, 7, 18),\n",
        "    catchup=False\n",
        ")\n",
        "\n",
        "trigger_dag_run = TriggerDagRunOperator(\n",
        "    task_id='trigger_dag_run',\n",
        "    trigger_dag_id='target_dag_id',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "task_a = DummyOperator(\n",
        "    task_id='task_a',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "task_b = DummyOperator(\n",
        "    task_id='task_b',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "task_a >> trigger_dag_run\n",
        "trigger_dag_run >> task_b\n"
      ],
      "metadata": {
        "id": "vY7zdmDI6vOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Sqoop\n",
        "1. Scenario: You want to import data from an Oracle database into Hadoop using Sqoop, but you only need to import specific columns from a specific table. Write a Sqoop command that performs the import, including the necessary arguments for column selection and table mapping.\n",
        "2. Scenario: You have a requirement to perform an incremental import of data from a MySQL database into Hadoop using Sqoop. Design a Sqoop command that imports only the new or updated records since the last import.\n",
        "3. Scenario: You need to export data from Hadoop to a Microsoft SQL Server database using Sqoop. Develop a Sqoop command that exports the data, considering factors like database connection details, table mapping, and appropriate data types.\n"
      ],
      "metadata": {
        "id": "GKKQWbVT66bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:oracle:thin:@<oracle_host>:<oracle_port>:<oracle_sid> \\\n",
        "--username <username> \\\n",
        "--password <password> \\\n",
        "--table <table_name> \\\n",
        "--columns \"<column1>,<column2>,<column3>\" \\\n",
        "--target-dir <target_directory>\n"
      ],
      "metadata": {
        "id": "lWZoIU8W67w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://<mysql_host>/<database_name> \\\n",
        "--username <username> \\\n",
        "--password <password> \\\n",
        "--table <table_name> \\\n",
        "--incremental append \\\n",
        "--check-column <timestamp_column> \\\n",
        "--last-value <last_import_timestamp> \\\n",
        "--target-dir <target_directory>\n"
      ],
      "metadata": {
        "id": "HszyBmRO7JHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqoop export \\\n",
        "--connect \"jdbc:sqlserver://<sql_server_host>:<sql_server_port>;database=<database_name>\" \\\n",
        "--username <username> \\\n",
        "--password <password> \\\n",
        "--table <table_name> \\\n",
        "--export-dir <export_directory> \\\n",
        "--input-fields-terminated-by ',' \\\n",
        "--input-lines-terminated-by '\\n' \\\n",
        "--input-null-string '\\\\N' \\\n",
        "--input-null-non-string '\\\\N'\n"
      ],
      "metadata": {
        "id": "UdIwQ_Na7MZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}