{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Working with RDDs:\n",
        "   a) Write a Python program to create an RDD from a local data source.\n",
        "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
        "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
        "\n"
      ],
      "metadata": {
        "id": "cafZz1CWwJjB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq_Cu7e2wIpp"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"RDD Example\")\n",
        "\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "rdd.collect()\n",
        "\n",
        "rdd_mapped = rdd.map(lambda x: x * 2)\n",
        "\n",
        "rdd_filtered = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "sum_of_elements = rdd.reduce(lambda x, y: x + y)\n",
        "\n",
        "sum_count = rdd.aggregate((0, 0), lambda acc, value: (acc[0] + value, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
        "\n",
        "print(rdd_mapped.collect())\n",
        "print(rdd_filtered.collect())\n",
        "print(sum_of_elements)\n",
        "print(sum_count)\n",
        "\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)\n",
        "\n",
        "filtered_rdd = rdd.filter(lambda x: x > 3)\n",
        "\n",
        "max_number = rdd.reduce(lambda x, y: x if x > y else y)\n",
        "\n",
        "sum_count = rdd.aggregate((0, 0), lambda acc, value: (acc[0] + value, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
        "average = sum_count[0] / sum_count[1]\n",
        "\n",
        "print(squared_rdd.collect())\n",
        "print(filtered_rdd.collect())\n",
        "print(max_number)\n",
        "print(average)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Spark DataFrame Operations:\n",
        "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
        "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
        "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QKkjfgQCwavc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"file_path.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.show()\n",
        "\n",
        "filtered_df = df.filter(df[\"age\"] > 30)\n",
        "\n",
        "grouped_df = df.groupBy(\"gender\").agg({\"age\": \"mean\", \"salary\": \"sum\"})\n",
        "\n",
        "joined_df = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")\n",
        "\n",
        "filtered_df.show()\n",
        "grouped_df.show()\n",
        "joined_df.show()\n",
        "\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")\n",
        "\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "hFvCMBTAwdQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Spark Streaming:\n",
        "  a) Write a Python program to create a Spark Streaming application.\n",
        "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
        "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
        "\n"
      ],
      "metadata": {
        "id": "O0zlW4o_wdtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "ssc = StreamingContext(sparkContext, 1)\n",
        "\n",
        "dstream = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "processed_dstream = dstream.map(lambda line: line.split(\" \")).filter(lambda words: \"error\" in words)\n",
        "\n",
        "processed_dstream.pprint()\n",
        "\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "QtiV9HYkwf7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Spark SQL and Data Source Integration:\n",
        "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
        "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
        "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3."
      ],
      "metadata": {
        "id": "yaWpq3Sewjiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Example\").getOrCreate()\n",
        "\n",
        "jdbc_url = \"jdbc:mysql://localhost:3306/database_name\"\n",
        "db_properties = {\n",
        "  \"user\": \"username\",\n",
        "  \"password\": \"password\"\n",
        "}\n",
        "\n",
        "df = spark.read.jdbc(url=jdbc_url, table=\"table_name\", properties=db_properties)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "result = spark.sql(\"SELECT * FROM table WHERE column > 10\")\n",
        "\n",
        "result.show()\n",
        "\n",
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "result = spark.sql(\"SELECT * FROM table WHERE column > 10\")\n",
        "\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "KdhCc-GQwknT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}